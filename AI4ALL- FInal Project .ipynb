{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8775f6",
   "metadata": {},
   "source": [
    "## Major steps \n",
    "1. Set up a Super Mario Bros game Enviornment \n",
    "2. Preocess the game for Reinforcement Learning \n",
    "3. Train the model \n",
    "4. Save and load the trained model in different enviornments \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b87097",
   "metadata": {},
   "source": [
    "## Tools used (add as needed)\n",
    "---\n",
    "### OpenAI Gym\n",
    "- describe\n",
    "### Mario Gym\n",
    "- describe\n",
    "### Nes-Py\n",
    "- describe\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2384a31",
   "metadata": {},
   "source": [
    "## Step 1. Set up Super Mario Bros\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52832b",
   "metadata": {},
   "source": [
    "## Install dependencies and import needed libraries \n",
    "---\n",
    "Install the needed dependencies of the Super Mario Gym and the Nes-py Emulator. Additonally import the needed libraries in order to build the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8323c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gym_super_mario_bros==7.3.0 nes_py \n",
    "print(\"Done setting up initial dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c74c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the game gym\n",
    "import gym_super_mario_bros\n",
    "#import the Joypad Wrapper \n",
    "from nes_py.wrappers import JoypadSpace\n",
    "#import the controls (simplified)\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4e8a0",
   "metadata": {},
   "source": [
    "It is very important to have simplfied inputs for the AI as more complicated inputs can lead to it having a harder time learning, to show this we can observe the actions Mario can take by viewing the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dd4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00269be",
   "metadata": {},
   "source": [
    "As we can see Mario can only take 7 actions (note: NOOP means no operation/ do nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the game\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0') #create the game gym (world 1-1)\n",
    "env = JoypadSpace(env,SIMPLE_MOVEMENT) #simplify the number of inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d60799",
   "metadata": {},
   "source": [
    "To check how many actions Mario can take we can simply view it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33219449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c20f1",
   "metadata": {},
   "source": [
    "## Play the game \n",
    "---\n",
    "- outline\n",
    "    - playing the game to test the enviornment\n",
    "        - congrats python can play the game\n",
    "    - print out the results of each frame \n",
    "    - try it yourself option (change the number of steps to take)\n",
    "    - note how long it takes if the standard 10,000 steps are taken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd0bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "done = True #check if Mario has finished a stage and to restart the session \n",
    "\n",
    "for step in range(100): #should be 100000\n",
    "    score = 0\n",
    "    if done:\n",
    "        #start the game\n",
    "        env.reset()\n",
    "    #get information on each step/frame of the game and also allow mario to take a random action \n",
    "    state,reward,done,info = env.step(env.action_space.sample())\n",
    "    score+=reward\n",
    "    #show the state of each frame /step \n",
    "    \n",
    "    #this will create a vary long output feel free to uncomment the print statement below\n",
    "    print('Frame/step:{} Score:{} done:{}'.format(step,score,done)) \n",
    "\n",
    "    #render the gym (leave commented for faster completion time)\n",
    "    #env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2766cc3",
   "metadata": {},
   "source": [
    "Since at this stage Mario has not been trained it is likely he will simply be caught on the first pipe until the timer runs out\n",
    "![image](/img/mario_cannot_jump_high.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ce123",
   "metadata": {},
   "source": [
    "## Step 2. Pre-processing the enviornment\n",
    "---\n",
    "In this step the game's data needs to be proprocessed in order to eliminate any garbage data.\n",
    "This will involve two steps\n",
    "1. Convert the image to grayscale in order to simply the data even more \n",
    "2. Stack frames together so the agent can read/remember it's previous steps instead of acting on a frame-by-frame basis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5ed44",
   "metadata": {},
   "source": [
    "### install and import more dependencies and libaries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8f545",
   "metadata": {},
   "source": [
    "### Installing pytorch \n",
    "- This will take a few minutes to install please be patient \n",
    "- explain what pytorch is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbc740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install pytorch\n",
    "!pip3 install torch torchvision torchaudio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318880e",
   "metadata": {},
   "source": [
    "\n",
    "### installing additional Reinforcement learning dependencies \n",
    "- This will take a few minutes to install please be patient \n",
    "- expain the need for the PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af7db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Frame Stacker Wrapper and GrayScaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "# Import Vectorization Wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "# Import Matplotlib to show the impact of frame stacking\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a18527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create the enviornment \n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "# 2. Simplify the controls \n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15943d37",
   "metadata": {},
   "source": [
    "### **The importance of simplifying the enveiornment**\n",
    "---\n",
    "As we continue in pre-processing we must convert the enviornment from RGB to grayscale to feed the agent as simple of an enviornment as possible \n",
    "\n",
    "To show the difference compare the enviornment with and without the grayscale conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7e04d",
   "metadata": {},
   "source": [
    "### No Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a376c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the base environment (no pre-processing)\n",
    "env_no_pre = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env_no_pre = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env_no_pre_state = env_no_pre.reset()\n",
    "plt.imshow(env_no_pre_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa98c0",
   "metadata": {},
   "source": [
    "We can inspect the shape to see how much information needs to be proccesed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70639e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env_no_pre_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e416491",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the base environment (with pre processing)\n",
    "env_w_pre = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env_w_pre = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env_w_pre = GrayScaleObservation(env_w_pre, keep_dim=True)\n",
    "env_w_pre_state = env_w_pre.reset()\n",
    "plt.imshow(env_w_pre_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c637f",
   "metadata": {},
   "source": [
    "Let us inpsect the shape of the envionrment now after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22450bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env_w_pre_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad1914",
   "metadata": {},
   "source": [
    "In doing so the 3rd element in the shape list went from 3->1 channels thus we have to only process the monochromatic parts\n",
    "of the enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfe464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Grayscale\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "#4. Wrap inside the dummy enviornment\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# 5. Stack the frames\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552fa226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "state = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step([5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424e106",
   "metadata": {},
   "source": [
    "In this case it is hard to explain what the stacked frame does, however stacked frames alows the agent to see up to 4 of the previous frames in order to gain a better insight on what to do. When shown it would look like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e2db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#display 4 frames \n",
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(state[0][:,:,idx])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de76570",
   "metadata": {},
   "source": [
    "The first 2 frames are purple due to the game not advancing past the intial 2 frames, however if we step through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecace2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step([5]) #run this as many times as you want to see the frames advance \n",
    "plt.figure(figsize=(30,32))\n",
    "for idx in range(state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b665ed",
   "metadata": {},
   "source": [
    "### Step 3. Training the Reinforcement Model\n",
    "---\n",
    "This step will be all about training the AI to navigate the enviorntment. This uses reinforcemt learning with Mario\n",
    "as the agent. Mario will be rewarded for for how far right he makes it in the enviornment. The goal of Mario is to maximize his reward.\n",
    "\n",
    "The Reinforcement Alogirithm being used is the PPO (Proximal Policy Optimization) algorithm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fe361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os \n",
    "# Import PPO for algos\n",
    "from stable_baselines3 import PPO\n",
    "# Import Base Callback for saving models to prevent massive data loss \n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd67a72",
   "metadata": {},
   "source": [
    "Callback class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c9ab6",
   "metadata": {},
   "source": [
    "### Sources: \n",
    "- “Gym-super-mario-bros,” PyPI. [Online]. Available: https://pypi.org/project/gym-super-mario-bros/. [Accessed: 03-May-2022]. \n",
    "- J. Schulman, “Proximal policy optimization,” OpenAI, 02-Sep-2020. [Online]. Available: https://openai.com/blog/openai-baselines-ppo/. [Accessed: 04-May-2022]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a841b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
